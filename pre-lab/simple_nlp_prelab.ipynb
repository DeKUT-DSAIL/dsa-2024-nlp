{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeKUT-DSAIL/DSA-2024-NLP/blob/main/pre-lab/simple_nlp_prelab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vG0i7XP3-h3"
      },
      "source": [
        "# Natural Language Processing (NLP)\n",
        "## What is Natural Language Processing?\n",
        "NLP is the study of how computers can interact with humans in natural human language, according to [[3]](https://www.oracle.com/ke/artificial-intelligence/what-is-natural-language-processing/).\n",
        "\n",
        "## Introduction\n",
        "In this lesson, we shall consider some foundational concepts in Natural Language Processing like stemming, lemmatization and vectorization of text (converting text data into vectors).\n",
        "\n",
        "## Objectives\n",
        "* Distinguish between stemming and lemmatization.\n",
        "* Describe stopwords and why they are removed.\n",
        "* Describe tokenization in the context of NLP.\n",
        "* Understand various vectorization techniques.\n",
        "* Use probability to predict the next word in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okn6QUeW6Vuj"
      },
      "source": [
        "# Basic NLP Lab\n",
        "In this section, we shall practice some basic NLP tasks, below are some of the terms that will be used in the lab:\n",
        "\n",
        "## Corpus\n",
        "A corpus is a structured set of texts that is machine-readable and sampled to be representative of a natural language. It is mostly used for linguistic research and analysis [4].\n",
        "\n",
        "## Document\n",
        "In NLP a document is a single unit of text that can be part of a corpus. It could be a tweet, an email, a report, an article, a book, etc.\n",
        "\n",
        "## Creating a Bag of words\n",
        "When working with text data, it is recommended that one vectorizes the text by creating a bag of words. Bag of words (BoW) is a way of representing text as an unordered collection that disregards grammar but retains the mutiplicity of the words i.e the number of times a word occurs in a text. This can be achieved through tokenization.\n",
        "\n",
        "## Basic Cleaning and Tokenization\n",
        "Using the sentence : \"Apple shareholders have had a great year. Apple's stock price has gone steadily upwards -- Apple even broke a trillion-dollar valuation, continuing the dominance of this tech stock.\" as our example.\n",
        "Consider capitalization, punctuation. e.g a computer would consider Apple and Apple's as different.\n",
        "\n",
        "## Stemming Vs. Lemmatization and Stopwords\n",
        "For smaller datasets, stemming, lemmatization and stopwords removal could be helpful in reducing the dimensionality of the data.\n",
        "Stemming and lemmatization are text preprocessing techniques that are used in NLP to reduce the inflected forms of words across across a text data set to one common root word. For example, the word sing can have affixated forms like sang, sung, singing, song, songs and singer. Another word dance, can have dancing, danced, dances and dancer.\n",
        "\n",
        "### Stemming\n",
        "Stemmming is the elimination of suffixes in words to return it to its root form. To illustrate, we know that adding and 's' to the end of a word makes it plural, if a stemming algorithm was given the word \"dogs\", it would return \"dog\". Stems do not have to make actual English sense. For example, \"historical\" would be reduced to \"histori\" and not \"history\".\n",
        "\n",
        "### Lemmatization\n",
        "Lemmatization differs from stemming in that it considers the morphology of the word (structure) and attempts to reduce each word into its most basic form (lemma) that can be found in the English dictionary. For example, \"historical\" would be reduced to \"history\".\n",
        "\n",
        "## Stopwords\n",
        "These are words that contain little or no actual information. Exmples include words like:\n",
        "* to\n",
        "* the\n",
        "* of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLr4CbmTH2Nb",
        "outputId": "8ca08209-4e10-4ab9-e66b-5b37d6a32906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzLHDp383h1h"
      },
      "outputs": [],
      "source": [
        "# Load dependencies\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('gutenberg', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "from nltk.corpus import gutenberg, stopwords, wordnet\n",
        "from nltk import FreqDist, word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.util import trigrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import string\n",
        "import pathlib\n",
        "import math\n",
        "import contractions\n",
        "from itertools import islice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYjuWShEsZKi"
      },
      "source": [
        "In the code cells below, we are going to use **The Great Gatsby** by **F. Scott Fitzgerald**, **The Adventures of Sherlock Holmes** by **Athur Conan Doyle** and **Adventures of Huckleberry Finn** by **Mark Twain**. These books are freely available at [Project Gutenberg](https://www.gutenberg.org/), we have scraped them from the internet .\n",
        "\n",
        "Demonstrations shall be done using the text from The Great Gatsby and the practice shall be done using the text from Adventures Of Huckleberry Finn or The Adventures of Sherlock Holmes.\n",
        "\n",
        "The libraries that shall be used heavily in this lab are natural language toolkit ([nltk](https://www.nltk.org/)), reguar expression operations ([re](https://docs.python.org/3/library/re.html)) and [spacy](https://spacy.io/). In order to use spacy more effectively, kindly use a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmUcmbFRGK9e",
        "outputId": "b1796e04-1f43-4ca9-a691-9daef970c9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Great Gatsby:\n",
            "I    In my younger and more vulnerable years my father gave me some advice  that I’ve been turning over in my mind ever since.    “Whenever you feel like criticizing anyone,” he told me, “just  remember that all the people in this world haven’t had the advantages  that you’ve had.”    He didn’t say any more, but we’ve always been unusually communicative  in a reserved way, and I understood that he meant a great deal more  than that. In consequence, I’m inclined to reserve all judgements, a  habi\n",
            "====================================================================================================\n",
            "Adventures of Huckleberry Finn:\n",
            "CHAPTER I.      You don’t know about me without you have read a book by the name of The  Adventures of Tom Sawyer; but that ain’t no matter. That book was made  by Mr. Mark Twain, and he told the truth, mainly. There was things  which he stretched, but mainly he told the truth. That is nothing. I  never seen anybody but lied one time or another, without it was Aunt  Polly, or the widow, or maybe Mary. Aunt Polly—Tom’s Aunt Polly, she  is—and Mary, and the Widow Douglas is all told about in that \n",
            "====================================================================================================\n",
            "The Adventures of Sherlock Holmes:\n",
            "A SCANDAL IN BOHEMIA      I.    To Sherlock Holmes she is always _the_ woman. I have seldom heard him  mention her under any other name. In his eyes she eclipses and  predominates the whole of her sex. It was not that he felt any emotion  akin to love for Irene Adler. All emotions, and that one particularly,  were abhorrent to his cold, precise but admirably balanced mind. He  was, I take it, the most perfect reasoning and observing machine that  the world has seen, but as a lover he would have \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "def request_gutenberg(url):\n",
        "\n",
        "  # Make a request to the url\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "    book = response.text\n",
        "\n",
        "  else:\n",
        "    print(f\"Failed to retrieve the text version. Status code: {response.status_code}\")\n",
        "\n",
        "  # remove unwanted new line and tab characters from the text, replacing with whitespace\n",
        "  unwanted_chars = [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]\n",
        "  for char in unwanted_chars:\n",
        "      book = book.replace(char, \" \")\n",
        "\n",
        "  return book\n",
        "\n",
        "# URLs of the books to be used in this lab\n",
        "g_gatsby_url = \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\"\n",
        "huckleberry_finn_url = \"https://www.gutenberg.org/cache/epub/76/pg76.txt\"\n",
        "sherlock_holmes_url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "\n",
        "# Make requests for each book\n",
        "g_gatsby = request_gutenberg(g_gatsby_url)[1494:277912] # Remove introduction and footnotes\n",
        "h_berry = request_gutenberg(huckleberry_finn_url)[9528:-18862]\n",
        "s_holmes = request_gutenberg(sherlock_holmes_url)[1508:-18859]\n",
        "\n",
        "books = {\n",
        "    \"The Great Gatsby\": g_gatsby,\n",
        "    \"Adventures of Huckleberry Finn\": h_berry,\n",
        "    \"The Adventures of Sherlock Holmes\": s_holmes\n",
        "    }\n",
        "\n",
        "# Preview\n",
        "for key, val in books.items():\n",
        "  print(f\"{key}:\")\n",
        "  print(val[:500])\n",
        "  print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8bP_j_PkUVz"
      },
      "outputs": [],
      "source": [
        "def fix_encoding(text):\n",
        "    # Encode the incorrectly decoded text back to bytes\n",
        "    bytes_text = text.encode('latin1')\n",
        "    # Decode the bytes using the correct encoding (UTF-8)\n",
        "    corrected_text = bytes_text.decode('utf-8')\n",
        "    return corrected_text\n",
        "\n",
        "# Fix encoding of sherlock holmes\n",
        "s_holmes = fix_encoding(s_holmes)\n",
        "s_holmes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS2ennHvBbQK"
      },
      "source": [
        "## Frequency Distributions\n",
        "The  frequency distributions of words in a document can be extracted to know which words appear more\\less frequently in a document. But in order to achieve this, the text must first be tokenized. Tokenization is basically breaking down the document into smaller pieces known as tokens which can be words or characters. For example, \"Hello, world!\" might result in the tokens [\"Hello\", \",\", \"\" , \"world\", \"!\".].\n",
        "\n",
        "In order to consider the value of the information captured in the tokens and also reduce the dimensionality of the data, one might consider to lemmatize (lemmatization is preferred because it considers the morphology in the text), remove stopwords and punctuation in the text.\n",
        "\n",
        "Depending on the source of the text and the use case, one might consider removing links and non-word characters like emoticons.\n",
        "\n",
        "**N/B**: Cleaning of the text such as removal of stopwords and numbers or even conversion of numbers in digit form to word form ought to be thought through carefully, after carefully considering the task at hand.\n",
        "\n",
        "To get familiar with regex pattern, we recommend that you could experiment with [regex 101](https://regex101.com/) and consider some cheatsheets like [datacamp](https://images.datacamp.com/image/upload/v1665049611/Marketing/Blog/Regular_Expressions_Cheat_Sheet.pdf) and [dataquest](https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf) to start with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDaadYDjFUXL",
        "outputId": "0b1c3348-a74c-48ec-fdee-179d3c31c9a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('the', 2397),\n",
              " ('a', 1684),\n",
              " ('and', 1568),\n",
              " ('i', 1391),\n",
              " ('to', 1130),\n",
              " ('of', 1119),\n",
              " ('he', 854),\n",
              " ('in', 808),\n",
              " ('wa', 768),\n",
              " ('it', 646)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_text(text):\n",
        "    pattern = r\"[a-zA-Z]+(?:'[a-z]+)?\"  # Regex pattern to match words, including those with apostrophes\n",
        "    text_tokens = nltk.regexp_tokenize(text, pattern)  # Tokenize using the regex pattern\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text_tokens = [lemmatizer.lemmatize(word.lower().strip()) for word in text_tokens]  # Convert tokens to lowercase, lemmatize, remove extra whitespace\n",
        "    return text_tokens\n",
        "\n",
        "g_gatsby_tokens = tokenize_text(g_gatsby) # tokenize the text\n",
        "g_gatsby_freqdist =FreqDist(g_gatsby_tokens) # Get frequency distributions\n",
        "g_gatsby_freqdist.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEHoHPufTmlF"
      },
      "source": [
        "Notice how a good number of the 10 most common words are words like 'the', 'and', 'of', and 'to'. These are stopwords which do not contain any meaningful information and they can be removed to reduce the dimensionality of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoGHjmYyaXTI"
      },
      "source": [
        "The function to tokenize text has already been made for you, practice using the text from huckleberry finn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpsEEmG4aVpE"
      },
      "outputs": [],
      "source": [
        "# Using Adventures of Huckelberry Finn / The Adventures of Sherlock Holmes\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROMOUszBt4A"
      },
      "source": [
        "## Removing stopwords and punctuation\n",
        " Removal of stopwords helps in the reducing the dimensionality of the data. Words like 'the', 'and', 'of' are considered as stopwords because they contain little to no information, thus they are removed.\n",
        "\n",
        " In the code cells below:\n",
        " 1. we shall create a list of stopwords from nltk's stopwords, then get the frequency distribution of the words in the text having already removed stopwords. What are the top 10 most common tokens in the Great Gatsby?\n",
        " 2. We shall create a function called clean text to clean the Great Gatsby text. This function should deal with contractions like \"don't\", lowercase the text, remove non-word characters taking into account possessives, remove stopwords and extra whitespace, and lemmatize the text (not necessarily in this order), but instead of returning it as tokens, it should be returned as text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq_w-oqHFFtW",
        "outputId": "9dfc89b7-9e6d-466b-a3cd-95fadc2c2f13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('wa', 768),\n",
              " ('gatsby', 263),\n",
              " ('said', 235),\n",
              " ('tom', 191),\n",
              " ('daisy', 186),\n",
              " ('one', 155),\n",
              " ('like', 122),\n",
              " ('mr', 115),\n",
              " ('man', 114),\n",
              " ('back', 109)]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords_list = stopwords.words('english') # List of stopwords\n",
        "\n",
        "g_gatsby_stopped = [word for word in g_gatsby_tokens if word not in stopwords_list] # Exclude stopwords and punctuation\n",
        "g_gatsby_stopped_freq = FreqDist(g_gatsby_stopped) # Get frequency distribution\n",
        "g_gatsby_stopped_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDjfwdpQa6WU"
      },
      "outputs": [],
      "source": [
        "# Using Adventures of Huckleberry Finn/The Adventures of Sherlock Holmes\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGt7T4tsLZ53"
      },
      "outputs": [],
      "source": [
        "# Using spacy for nlp\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpPZ29vgJrcr"
      },
      "outputs": [],
      "source": [
        "# Using spacy\n",
        "def clean_text1(text):\n",
        "  text = contractions.fix(text) # Fix contractions\n",
        "  text = text.lower() # lowercase the text\n",
        "  text = text.replace('\\n', ' ') # replace new line characters with whitespace\n",
        "  text = re.sub(r\"[^\\w\\s']\", ' ', text) # Remove non-word characters, except apostroophes - to handle possessives correctly e.g \"John's\"\n",
        "  text = text.strip() # Remove extra whitespace\n",
        "\n",
        "  ## -- Tokenize, lemmatize, remove stopwords -- ##\n",
        "  doc = nlp(text)\n",
        "  tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct] # Lemmatize and exclude stopwords\n",
        "  text = \" \".join(tokens)\n",
        "  # Remove any additional extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaSA1ny6k4db"
      },
      "outputs": [],
      "source": [
        "# Function to map NLTK Parts of Speech (POS) tags to WordNet POS tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxwPecUXYcAM"
      },
      "outputs": [],
      "source": [
        "# Using nltk\n",
        "def clean_text2(text):\n",
        "  # Fix contractions\n",
        "  text = contractions.fix(text)\n",
        "  text = text.lower() # lowercase\n",
        "  text = text.replace('\\n', ' ') # replace new line characters\n",
        "\n",
        "  # Remove non-word chaaraacters except apostrophe's and handle possessives\n",
        "  text = re.sub(r\"[^\\w\\s']\", ' ', text)\n",
        "  text = text.strip() # Remove extra whitespace\n",
        "\n",
        "  # Tokenize using regex pattern that captures words and common contractions/possessives\n",
        "  pattern = r\"[a-zA-Z]+(?:'[a-z]+)?\"\n",
        "  stopwords_list = set(stopwords.words('english'))\n",
        "  text_tokens = nltk.regexp_tokenize(text, pattern)  # Tokenize using the regex pattern\n",
        "  text_tokens = [word for word in text_tokens if word not in stopwords_list]\n",
        "\n",
        "  # Tag parts of speech\n",
        "  pos_tags = nltk.pos_tag(text_tokens)\n",
        "\n",
        "  # Lemmatize based on POS tags\n",
        "  lemmatizer = WordNetLemmatizer() # Init Lemmatizer\n",
        "  lemmatized_tokens = []\n",
        "  for word, tag in pos_tags:\n",
        "    wn_tag = get_wordnet_pos(tag)\n",
        "    if wn_tag:\n",
        "      lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wn_tag))\n",
        "    else:\n",
        "      lemmatized_tokens.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "  cleaned_text = \" \".join(lemmatized_tokens)\n",
        "  cleaned_text = re.sub(r\"\\s+\", ' ', cleaned_text).strip() # Remove any extra whitespace\n",
        "\n",
        "  return cleaned_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEEtCUjtKgB5",
        "outputId": "15ef8721-7682-4d86-87a0-3f70dbbba8f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "young vulnerable year father give advice turn mind feel like criticize tell remember people world advantage unusually communicative reserve way understand mean great deal consequence inclined reserve \n"
          ]
        }
      ],
      "source": [
        "# Clean the great gatbsy using the first function\n",
        "g_gatsby_cleaned1 = clean_text1(g_gatsby)\n",
        "print(g_gatsby_cleaned1[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV8LTXCdaidF",
        "outputId": "4aa53a1d-c008-4c56-9ccb-d05dad8913a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "young vulnerable year father give advice turning mind ever since whenever feel like criticize anyone tell remember people world advantage say always unusually communicative reserve way understood mean\n"
          ]
        }
      ],
      "source": [
        "# clean the great gatsby using the second function\n",
        "g_gatsby_cleaned2 = clean_text2(g_gatsby)\n",
        "print(g_gatsby_cleaned2[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22nnQdevooht"
      },
      "outputs": [],
      "source": [
        "# Using either/both functions clean the text of Adentures of Huckleberry Finn/The Adventures of Sherlock Holmes\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCUAYmc1XPXG"
      },
      "source": [
        "## N-grams\n",
        "\n",
        "N-grams are continuous sequences of n words from a given text. Types of n-grams include:\n",
        "* Unigrams - 1 word e.g ('God'). This can also be viewed as frequency distribution.\n",
        "* Bigrams - 2 words e.g ('Lord', 'God')\n",
        "* Trigrams - 3 words e.g ('Lord', 'God', 'said')\n",
        "\n",
        "In the code cell below, find the bigrams and trigams of the tokenized text (which excludes stopwords) that you created above.\n",
        "\n",
        "What are the top 10 for both bigram and trigram collections?\n",
        "\n",
        "Hint: Nltk's collocations provides an easy way to find these bigrams and trigrams. You could also use your own custom code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8A9A54NFFav",
        "outputId": "c0edfba9-ca32-48d1-ea06-ff7c40379dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('old', 'sport', 'said'), 0.0002464268112370626),\n",
              " (('said', 'mr', 'wolfshiem'), 0.0002464268112370626),\n",
              " (('doctor', 'j', 'eckleburg'), 0.0002053556760308855),\n",
              " (('new', 'york', 'wa'), 0.0002053556760308855),\n",
              " (('west', 'egg', 'village'), 0.0002053556760308855),\n",
              " (('eye', 'doctor', 'j'), 0.0001642845408247084),\n",
              " (('look', 'old', 'sport'), 0.0001642845408247084),\n",
              " (('oh', 'ga', 'od'), 0.0001642845408247084),\n",
              " (('copy', 'town', 'tattle'), 0.0001232134056185313),\n",
              " (('demanded', 'tom', 'suddenly'), 0.0001232134056185313),\n",
              " (('first', 'time', 'saw'), 0.0001232134056185313),\n",
              " (('ga', 'od', 'oh'), 0.0001232134056185313),\n",
              " (('long', 'time', 'ago'), 0.0001232134056185313),\n",
              " (('longest', 'day', 'year'), 0.0001232134056185313),\n",
              " (('od', 'oh', 'ga'), 0.0001232134056185313),\n",
              " (('two', 'young', 'woman'), 0.0001232134056185313),\n",
              " (('absolutely', 'real', 'page'), 8.21422704123542e-05),\n",
              " (('ah', 'h', 'h'), 8.21422704123542e-05),\n",
              " (('always', 'watch', 'longest'), 8.21422704123542e-05),\n",
              " (('another', 'bad', 'driver'), 8.21422704123542e-05),\n",
              " (('anyhow', 'demanded', 'tom'), 8.21422704123542e-05),\n",
              " (('ash', 'grey', 'men'), 8.21422704123542e-05),\n",
              " (('back', 'gatsby', 'face'), 8.21422704123542e-05),\n",
              " (('baker', 'sitting', 'table'), 8.21422704123542e-05),\n",
              " (('beg', 'pardon', 'said'), 8.21422704123542e-05),\n",
              " (('bles', 'sed', 'pre'), 8.21422704123542e-05),\n",
              " (('bottle', 'whisky', 'towel'), 8.21422704123542e-05),\n",
              " (('broad', 'flat', 'hand'), 8.21422704123542e-05),\n",
              " (('call', 'montauk', 'point'), 8.21422704123542e-05),\n",
              " (('came', 'gatsby', 'house'), 8.21422704123542e-05),\n",
              " (('chicago', 'wa', 'calling'), 8.21422704123542e-05),\n",
              " (('chin', 'raised', 'little'), 8.21422704123542e-05),\n",
              " (('conversation', 'driving', 'car'), 8.21422704123542e-05),\n",
              " (('daisy', 'change', 'mine'), 8.21422704123542e-05),\n",
              " (('daisy', 'said', 'tom'), 8.21422704123542e-05),\n",
              " (('dan', 'cody', 'yacht'), 8.21422704123542e-05),\n",
              " (('day', 'would', 'suit'), 8.21422704123542e-05),\n",
              " (('day', 'year', 'miss'), 8.21422704123542e-05),\n",
              " (('eye', 'looked', 'back'), 8.21422704123542e-05),\n",
              " (('fifty', 'year', 'old'), 8.21422704123542e-05),\n",
              " (('five', 'year', 'ago'), 8.21422704123542e-05),\n",
              " (('fixed', 'world', 'series'), 8.21422704123542e-05),\n",
              " (('fool', 'fool', 'god'), 8.21422704123542e-05),\n",
              " (('gatsby', 'called', 'phone'), 8.21422704123542e-05),\n",
              " (('gatsby', 'house', 'summer'), 8.21422704123542e-05),\n",
              " (('gatsby', 'house', 'wa'), 8.21422704123542e-05),\n",
              " (('gatsby', 'west', 'egg'), 8.21422704123542e-05),\n",
              " (('gay', 'exciting', 'thing'), 8.21422704123542e-05),\n",
              " (('george', 'b', 'wilson'), 8.21422704123542e-05),\n",
              " (('get', 'married', 'church'), 8.21422704123542e-05)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Using nltk's collocations\n",
        "# Bigrams\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "g_gatsby_bigram_finder = BigramCollocationFinder.from_words(g_gatsby_stopped)\n",
        "g_gatsby_bigrams_scored = g_gatsby_bigram_finder.score_ngrams(bigram_measures.raw_freq)\n",
        "\n",
        "# Trigrams\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "g_gatsby_trigram_finder = TrigramCollocationFinder.from_words(g_gatsby_stopped)\n",
        "g_gatsby_trigrams_scored = g_gatsby_trigram_finder.score_ngrams(trigram_measures.raw_freq)\n",
        "g_gatsby_trigrams_scored[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQp4ebnzEReX",
        "outputId": "fe37935b-3622-4839-ce37-eca468008f9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('gatsby', 's', 'house'), 11),\n",
              " (('come', 'gatsby', 's'), 7),\n",
              " (('west', 'egg', 'village'), 5),\n",
              " (('doctor', 't', 'j'), 5),\n",
              " (('t', 'j', 'eckleburg'), 5),\n",
              " (('eye', 'doctor', 't'), 4),\n",
              " (('look', 'old', 'sport'), 4),\n",
              " (('world', 's', 'series'), 4),\n",
              " (('gatsby', 's', 'face'), 4),\n",
              " (('wilson', 's', 'body'), 4),\n",
              " (('oh', 'ga', 'od'), 4),\n",
              " (('daisy', 's', 'house'), 4),\n",
              " (('live', 'west', 'egg'), 3),\n",
              " (('long', 'day', 'year'), 3),\n",
              " (('demand', 'tom', 'suddenly'), 3),\n",
              " (('copy', 'town', 'tattle'), 3),\n",
              " (('mrs', 'wilson', 's'), 3),\n",
              " (('beg', 'pardon', 'mr'), 3),\n",
              " (('s', 'house', 'summer'), 3),\n",
              " (('gatsby', 's', 'door'), 3),\n",
              " (('gatsby', 's', 'drive'), 3),\n",
              " (('old', 'sport', 'gatsby'), 3),\n",
              " (('daisy', 's', 'face'), 3),\n",
              " (('daisy', 's', 'voice'), 3),\n",
              " (('gatsby', 's', 'party'), 3),\n",
              " (('long', 'time', 'ago'), 3),\n",
              " (('hot', 'hot', 'hot'), 3),\n",
              " (('gatsby', 's', 'eye'), 3),\n",
              " (('gatsby', 's', 'car'), 3),\n",
              " (('myrtle', 'wilson', 's'), 3),\n",
              " (('ga', 'od', 'oh'), 3),\n",
              " (('od', 'oh', 'ga'), 3),\n",
              " (('east', 'new', 'york'), 2),\n",
              " (('long', 'island', 'sound'), 2),\n",
              " (('know', 'mr', 'gatsby'), 2),\n",
              " (('broad', 'flat', 'hand'), 2),\n",
              " (('chin', 'raise', 'little'), 2),\n",
              " (('miss', 'baker', 's'), 2),\n",
              " (('gay', 'exciting', 'thing'), 2),\n",
              " (('grey', 'sun', 'strain'), 2),\n",
              " (('sun', 'strain', 'eye'), 2),\n",
              " (('watch', 'long', 'day'), 2),\n",
              " (('day', 'year', 'miss'), 2),\n",
              " (('miss', 'baker', 'sit'), 2),\n",
              " (('baker', 'sit', 'table'), 2),\n",
              " (('tom', 'glance', 'impatiently'), 2),\n",
              " (('telephone', 'rang', 'inside'), 2),\n",
              " (('butler', 's', 'nose'), 2),\n",
              " (('thing', 'bad', 'bad'), 2),\n",
              " (('woman', 'new', 'york'), 2)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using custom code - feel free to make your own version of such code\n",
        "def ngrams(tokens, n):\n",
        "  return zip(*[tokens[i:] for i in range(n)])\n",
        "\n",
        "def ngram_freq(text, n):\n",
        "  doc = nlp(text)\n",
        "  tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "  # Generate ngrams\n",
        "  ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "  # Count frequencies\n",
        "  ngrams_freq = Counter(ngrams_list)\n",
        "  return ngrams_freq.most_common()\n",
        "\n",
        "# get the ngram frequencies using custom code\n",
        "gatsby_trigrams = ngram_freq(g_gatsby_cleaned1, 3)\n",
        "gatsby_bigrams = ngram_freq(g_gatsby_cleaned1, 2)\n",
        "\n",
        "# Preview\n",
        "gatsby_trigrams[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS-n5e_IEqTD"
      },
      "outputs": [],
      "source": [
        "# Use nltk's collocations on Adventures of Huckleberry Finn/The Adventures of Sherlock Holmes to get ngrams\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wl8zjJBr8CX"
      },
      "outputs": [],
      "source": [
        "# Use custom code on Adventures of Huckleberry Finn/The Adventures of Sherlock Holmes to get ngrams\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwV0ErNTby5n"
      },
      "source": [
        "## Pointwise Mutual Information (PMI)\n",
        "Pointwise mutual information is a measure of association that compares the probability of two events ocurring together to what this probability would be if the events were independent [[5]](https://en.wikipedia.org/wiki/Pointwise_mutual_information).\n",
        "PMI helps us find related words, in simpler terms. It explains the co-occurence of 2 words together than we would expect by chance, it asks the question, \"does the occurrence of one word depend on the occurrence of another word?\"\n",
        "\n",
        "For example the word \"Data Science\" has a specific meaning when these two words \"Data\" and \"Science\" go together. Otherwise meaning of these two words are independent. Similarly \"Great Britain\" is meaningful since we know the word \"Great\" can be used with several other words but not so relevant in meaning like \"Great UK, Great London, Great Dubai etc.\"\n",
        "\n",
        "\n",
        "This can be expressed mathematically as:\n",
        "\\begin{equation}\n",
        "PMI(w_1, w_2) = log_2 \\frac{P(w_1, w_2)}{P(w_1)P(w_2)}\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "* $PMI(w_1,w_2)$ is the pointwise mutual information of word 1 and word2\n",
        "* $P(w_1,w_2)$ is the probability of word 1 and word 2 occurring together. This is considered as an intersection, $P(w_1 ∩ w_2)$.\n",
        "* $P(w_1)$ and $P(w_2)$ is the probability of word 1 and word 2 occurring individually.\n",
        "\n",
        "The probability of a word occuring can be calculated as:\n",
        "\\begin{equation}\n",
        "P(w) = \\frac{N(w)}{T}\n",
        "\\end{equation}\n",
        "where:\n",
        "* $N(w)$ is the frequency of word, w.\n",
        "* $T$ is the total word count.\n",
        "* $P(w)$ is the probability of word, w occurring.\n",
        "\n",
        "Find the PMI scores of the great_gatsby, use the tokens instead of the text.\n",
        "\n",
        "Hint: Use [NLTK's BigramCollocationFinder](https://www.nltk.org/api/nltk.collocations.BigramCollocationFinder.html#:~:text=A%20tool%20for%20the%20finding,than%20constructing%20an%20instance%20directly.&text=Construct%20a%20BigramCollocationFinder%2C%20given%20FreqDists,possibly%20non%2Dcontiguous%20bigrams.) for an easier way out, and apply a frequency filter of 5.\n",
        "\n",
        "One can also come up with their own function to calculate the PMI scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPfTOeWed8na",
        "outputId": "2d47eaf8-f033-4256-9f7e-69c196ccdbbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('beg', 'pardon'), 11.98655314975666),\n",
              " (('j', 'eckleburg'), 11.501126322586419),\n",
              " (('doctor', 'j'), 10.98655314975666),\n",
              " (('dan', 'cody'), 10.664625054869294),\n",
              " (('ash', 'heap'), 10.385649105166483),\n",
              " (('meyer', 'wolfshiem'), 9.349123229141368),\n",
              " (('miss', 'baker'), 8.756547544310335),\n",
              " (('west', 'egg'), 8.741792915391757),\n",
              " (('egg', 'village'), 8.44223263353285),\n",
              " (('new', 'york'), 8.400412457838224),\n",
              " (('long', 'island'), 8.032356839369784),\n",
              " (('old', 'sport'), 8.027195134254006),\n",
              " (('half', 'dozen'), 7.816628148314345),\n",
              " (('east', 'egg'), 7.72602559953344),\n",
              " (('shook', 'head'), 7.6427505552465345),\n",
              " (('mr', 'mckee'), 7.6329161951419575),\n",
              " (('living', 'room'), 7.6290011451385755),\n",
              " (('half', 'hour'), 7.601889299521332),\n",
              " (('mr', 'sloane'), 7.404097504646076),\n",
              " (('never', 'loved'), 7.249587555590452),\n",
              " (('mr', 'carraway'), 7.211452426703682),\n",
              " (('five', 'year'), 7.195139771568076),\n",
              " (('four', 'clock'), 7.193004027224086),\n",
              " (('man', 'named'), 7.153663135591918),\n",
              " (('green', 'light'), 7.078060449551641),\n",
              " (('shook', 'hand'), 6.995766327917991),\n",
              " (('mr', 'wolfshiem'), 6.813488440783781),\n",
              " (('good', 'night'), 6.719766609061759),\n",
              " (('yellow', 'car'), 6.669140535991788),\n",
              " (('jordan', 'baker'), 6.644725497431592),\n",
              " (('never', 'seen'), 6.617319340090939),\n",
              " (('go', 'bed'), 6.539194363419938),\n",
              " (('tom', 'buchanan'), 6.520155634109653),\n",
              " (('oxford', 'man'), 6.516233214976626),\n",
              " (('far', 'away'), 6.494700053426985),\n",
              " (('killed', 'man'), 6.4755912304792815),\n",
              " (('front', 'door'), 6.467572220586256),\n",
              " (('myrtle', 'wilson'), 6.366129654447057),\n",
              " (('jay', 'gatsby'), 6.269562255351721),\n",
              " (('young', 'men'), 6.2383603001672),\n",
              " (('next', 'door'), 6.059198451588424),\n",
              " (('get', 'married'), 6.004700496466921),\n",
              " (('next', 'day'), 5.986553149756659),\n",
              " (('go', 'home'), 5.819302282612673),\n",
              " (('mr', 'wilson'), 5.7071665722821265),\n",
              " (('let', 'go'), 5.641764097060484),\n",
              " (('walked', 'back'), 5.60169746453124),\n",
              " (('little', 'later'), 5.563087028407235),\n",
              " (('turned', 'head'), 5.467178990663081),\n",
              " (('turned', 'around'), 5.442232633532846)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PMI for the Great Gatsby\n",
        "gatsby_pmi_finder = BigramCollocationFinder.from_words(g_gatsby_stopped)\n",
        "gatsby_pmi_finder.apply_freq_filter(5) # use s as the frequency filter, meaning words that appear together\n",
        "gatsby_pmi_scored = gatsby_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
        "gatsby_pmi_scored[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XKL7CBWQEnL"
      },
      "source": [
        "# Next word probability\n",
        "\n",
        "In this section, we shall use probability to predict the next word in a sequence of text, given a bigram or a trigram.\n",
        "We can now calculate the probability of a word occurring given the two words that come before it.\n",
        "By using Maximum Likelihood Estimation (MLE), write a function called calc_ngram_prob that takes in a trigram, bigram_collection, trigram_collection and returns the probability of that trigram appearing. Use the formula:\n",
        "\\begin{equation}\n",
        "p(w_n|w_{n−2}w_{n−1}) = \\frac{C(w_{n−2}w_{n−1}w_n)}{C(w_{n−2}w_{n−1})}\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "* $p(w_n|w_{n−2}w_{n−1})$ is the probbility of the word $w_n$ occurring after the bigram $(w_{n−2}w_{n−1})$.\n",
        "* $C(w_{n−2}w_{n−1}w_n)$ is the count of the trigram $(w_{n−2}w_{n−1}w_n)$ in the corpus.\n",
        "* $C(w_{n−2}w_{n−1})$ is the count of the bigram $(w_{n−2}w_{n−1})$ in the corpus.\n",
        "\n",
        "This should give the likelihood of $w_n$ following $w_{n-2}$ and $w_{n-1}$.\n",
        "\n",
        "To ensure numerical stability, we'll operate with logarithmic probabilities. Encapsulate the division operation within a math.log function call to transform it into a logarithmic probability. Additionally, it's essential to address scenarios where the bigram or trigram is absent from the provided collection. In such cases, return negative infinity to indicate out-of-vocabulary n-grams.\n",
        "\n",
        "Try out the function on a couple of trigrams on your text of preference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJH4G4JsQIHh"
      },
      "outputs": [],
      "source": [
        "def calc_ngram_prob(trigram, bigram_collection, trigram_collection):\n",
        "    # Extract the components of the trigram\n",
        "    w1, w2, w3 = trigram\n",
        "    bigram_count = 0\n",
        "    trigram_count = 0\n",
        "\n",
        "    ## -- Check if the bigram and trigram exist in the collections -- ##\n",
        "\n",
        "    # Check that the bigram exists\n",
        "    bigram_found = False\n",
        "    for bigram, freq in bigram_collection:\n",
        "        if bigram == (w1, w2):\n",
        "            bigram_count = freq\n",
        "            bigram_found = True\n",
        "            break\n",
        "\n",
        "    # If the bigram is not found\n",
        "    if not bigram_found:\n",
        "        return float(\"-inf\")  # Return negative infinity for out-of-vocabulary bigrams\n",
        "\n",
        "    # Check that the trigram exists\n",
        "    trigram_found = False\n",
        "    for trigram_tuple, freq in trigram_collection:\n",
        "        if trigram_tuple == (w1, w2, w3):\n",
        "            trigram_count = freq\n",
        "            trigram_found = True\n",
        "            break\n",
        "\n",
        "    # If the trigram does not exist\n",
        "    if not trigram_found:\n",
        "        return float(\"-inf\")  # Return negative infinity for out-of-vocabulary trigrams\n",
        "\n",
        "    # Calculate the probability\n",
        "    if bigram_count == 0:\n",
        "        return float(\"-inf\")  # Return negative infinity for out-of-vocabulary bigrams\n",
        "    else:\n",
        "        log_prob = math.log(trigram_count / bigram_count) if trigram_count > 0 else float(\"-inf\")\n",
        "        return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfhxRfPhX7J5",
        "outputId": "08549d27-7aa0-49fd-dedb-5fffb977c44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.5686159179138452\n"
          ]
        }
      ],
      "source": [
        "# The next word probability using the trigram ('west', 'egg', 'village') from the Great Gatsby\n",
        "next_word_prob = calc_ngram_prob(('west', 'egg', 'village'), gatsby_bigrams, gatsby_trigrams)\n",
        "print(next_word_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuvW_vINrvQI"
      },
      "outputs": [],
      "source": [
        "# Use Adventures of Huckleberry Finn/ The Adventures of Sherlock Holmes to calculate ngram probability\n",
        "\n",
        "# Your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew82729rMnAv"
      },
      "outputs": [],
      "source": [
        "# Get the PMI of Adventures of Huckleberry Finn/The Adventures of Sherlock Holmes\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UusWlUSdU_R_"
      },
      "source": [
        "## Next word Prediction\n",
        "In the code cells below, we shall predict the next possible words by using probability. To do this, we create two functions:\n",
        "* The first function provides a list of the next possible words.\n",
        "* The second function provides he most likely next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK57RVCtVCUr"
      },
      "outputs": [],
      "source": [
        "def get_possible_next_words(sequence, bigram_collection, trigram_collection):\n",
        "    # Tokenize the input sequence\n",
        "    tokens = sequence.lower().split()\n",
        "\n",
        "    # Extract the last two words as the context for trigrams\n",
        "    context = tokens[-2:]\n",
        "\n",
        "    # Initialize a dictionary to store possible next words and their probabilities\n",
        "    possible_next_words = {}\n",
        "\n",
        "    # Iterate through the trigram collection\n",
        "    for trigram, freq in trigram_collection:\n",
        "        # Check if the first two words of the trigram match the context\n",
        "        if trigram[:2] == tuple(context):\n",
        "            next_word = trigram[2]  # Extract the next word from the trigram\n",
        "            # Calculate the probability of the next word given the context\n",
        "            prob = calc_ngram_prob(trigram, bigram_collection, trigram_collection)\n",
        "            # Update the dictionary with the next word and its probability\n",
        "            possible_next_words[next_word] = prob\n",
        "\n",
        "    # Sort the dictionary by probabilities in descending order\n",
        "    sorted_next_words = sorted(possible_next_words.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_next_words\n",
        "\n",
        "def predict_next_word(sequence, bigram_collection, trigram_collection):\n",
        "    # Get possible next words along with their probabilities\n",
        "    possible_next_words = get_possible_next_words(sequence, bigram_collection, trigram_collection)\n",
        "\n",
        "    # If no possible next words are found, return None\n",
        "    if not possible_next_words:\n",
        "        return None\n",
        "\n",
        "    # Return the most likely next word (the first element of the sorted list)\n",
        "    return possible_next_words[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RMBbIY-YCDn",
        "outputId": "b53b10eb-649a-44dc-b540-77b03504d7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "possible next words for 'old sport': \n",
            "[('gatsby', -2.70805020110221), ('near', -3.8066624897703196), ('afraid', -3.8066624897703196), ('urge', -3.8066624897703196), ('familiar', -3.8066624897703196), ('good', -3.8066624897703196), ('lunch', -3.8066624897703196), ('jump', -3.8066624897703196), ('break', -3.8066624897703196), ('great', -3.8066624897703196)]\n",
            "\n",
            "most likely next word for 'old sport': \n",
            "gatsby \n"
          ]
        }
      ],
      "source": [
        "# Get the possible next words and most likely next word for a text sequence\n",
        "text_seq = \"old sport\"\n",
        "n = 10 # set a limit for the number of possible next words\n",
        "\n",
        "# possible next words\n",
        "possible_next_word = get_possible_next_words(text_seq, gatsby_bigrams, gatsby_trigrams)\n",
        "print(f\"possible next words for \\'{text_seq}': \\n{possible_next_word[:n]}\")\n",
        "\n",
        "# most likely next word\n",
        "most_likely_next_word = predict_next_word(text_seq, gatsby_bigrams, gatsby_trigrams)\n",
        "print(f\"\\nmost likely next word for \\'{text_seq}': \\n{most_likely_next_word} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2hfFYMQb95U"
      },
      "outputs": [],
      "source": [
        "# Get the possible next words and most likely next word of a text sequence of your choice\n",
        "\n",
        "# You can use any text sequence from any corpus\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZL9TBhqUDF"
      },
      "source": [
        "# Word Embeddings (Optional)\n",
        "Word embeddings are numerical representations of words in a lower-dimensional space that capture their semantic and syntactic information. Word embeddings act as the digital DNA for words in natural language processing (NLP). Essentially, they transform words into numerical vectors (arrays of numbers) that machine learning algorithms can process.\n",
        "\n",
        "Imagine these vectors as numeric fingerprints for each word. For instance, the word \"apple\" might be represented by a vector like [0.1, -0.3, 0.6].They help machines grasp the meaning and nuances behind each word.\n",
        "\n",
        "For example, if \"apple\" is close to \"fruit\" in this numerical space but far from \"car,\" the machine understands that an apple is more related to fruits than to vehicles.\n",
        "\n",
        "Moreover, word embeddings encode relationships between words. Words that frequently appear together in the same context will have similar or 'closer' vectors.\n",
        "\n",
        "A classic example is, in the numerical space, the vectors for \"king\" and \"queen\" might be closer to each other than those for \"king\" and \"apple.\" This is because the algorithm has learned from vast amounts of text that \"king\" and \"queen\" often appear in similar contexts, such as discussions about royalty, while \"king\" and \"apple\" do not.\n",
        "\n",
        "![word_embeddings_space](https://www.freecodecamp.org/news/content/images/size/w1600/2023/09/Screenshot-2023-09-24-at-5.43.04-PM.png)\n",
        "\n",
        "\n",
        "## Vectorization\n",
        "\n",
        "ML algorithms do not understand text, but they do understand math and in turn vectors and matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKD--bVvqYtV"
      },
      "source": [
        "### Count Vectorization\n",
        "Vectorization of the text based on the counts of the words. Frequency of unique words is got and represented as a matrix, each row would represent a document (or a chunk of text, if the text is from one book). Each column would represent a unique word in the matrix. CountVectorizer uses a bag of words approach to create a vector representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY1nb0uyswLD"
      },
      "source": [
        "### Term Frequency, Inverse Document Frequency (TF-IDF)\n",
        "Weighs each term in a document by how unique it is to the given document it is contained in, this can allow us to create a summary of the document's contents using a few key words. In simple terms, it answers the question, \"which words are the most important in a document?\"\n",
        "The formula for term frequency is:\n",
        "\\begin{equation}\n",
        "TF(t) = \\frac{N}{T}\n",
        "\\end{equation}\n",
        "Where:\n",
        "* N is the Number of times t appears in a document\n",
        "* T is the total number of terms in the document\n",
        "\n",
        "The formula for inverse document frequency is:\n",
        "\n",
        "\\begin{equation}\n",
        "IDF(t) = log_e \\frac{N} {N_{with-t}}\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "* t is a term (word)\n",
        "* N is the total number of documents\n",
        "* $N_{with-t}$ is the number of documents with t in it\n",
        "\n",
        "In the code cells below, we shall use sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guz9yON60Ibl",
        "outputId": "bebb2a5a-9c27-4795-9ad0-a03edec9cf47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['young vulnerable year father give advice turn mind feel like criticize tell remember people world advantage unusually communicative reserve way understand mean great deal consequence inclined reserve ', 'judgement habit open curious nature victim veteran bore abnormal mind quick detect attach quality appear normal person come college unjustly accuse politician privy secret grief wild unknown man confidence unsought frequently feign sleep preoccupation hostile levity realize unmistakable sign intimate', 'revelation quiver horizon intimate revelation young man term express usually plagiaristic mar obvious suppression reserve judgement matter infinite hope little afraid miss forget father snobbishly suggest snobbishly repeat sense fundamental decency parcel unequally birth boast way tolerance come admission', 'limit conduct found hard rock wet marsh certain point care found come east autumn feel want world uniform sort moral attention forever want riotous excursion privileged glimpse human heart gatsby', 'gatsby man give book exempt reaction gatsby represent unaffected scorn personality unbroken series successful gesture gorgeous heighten sensitivity promise life relate intricate machine register earthquake ', 'housand mile away responsiveness flabby impressionability dignified creative temperament extraordinary gift hope romantic readiness find person likely shall find gatsby turn right end prey gatsby foul', 'dust float wake dream temporarily close interest abortive sorrow short wind elation man family prominent people middle western city generation carraway clan tradition descend duke buccleuch actual found']\n"
          ]
        }
      ],
      "source": [
        "# Create a corpus of documents from some texts based on the great gatsby - this is totally random feel free to use any corpus of your choice\n",
        "# The corpus is sampled in this way to allow for easy demonstration\n",
        "# each item in the below list is considered as a document\n",
        "corpus = [g_gatsby_cleaned1[:200],\n",
        "          g_gatsby_cleaned1[200:501],\n",
        "          g_gatsby_cleaned1[502:808],\n",
        "          g_gatsby_cleaned1[809:1004],\n",
        "          g_gatsby_cleaned1[998:1204],\n",
        "          g_gatsby_cleaned1[1205:1405],\n",
        "          g_gatsby_cleaned1[1406:1608]]\n",
        "\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNWz4mld39Cu"
      },
      "outputs": [],
      "source": [
        "# Create a corpus of your choice, use the cleaned text from Adventures of Huckleberry Finn/ The Adventures of Sherlock Holmes\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oWHUQeSqxxH",
        "outputId": "0f3803ec-ed04-4b58-a4be-4a1a4a2367e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Great Gatsby CountVectorizer feature names:\n",
            " ['abnormal' 'abortive' 'accuse' 'actual' 'admission' 'advantage' 'advice'\n",
            " 'afraid' 'appear' 'attach' 'attention' 'autumn' 'away' 'birth' 'boast'\n",
            " 'book' 'bore' 'buccleuch' 'care' 'carraway' 'certain' 'city' 'clan'\n",
            " 'close' 'college' 'come' 'communicative' 'conduct' 'confidence'\n",
            " 'consequence' 'creative' 'criticize' 'curious' 'deal' 'decency' 'descend'\n",
            " 'detect' 'dignified' 'dream' 'duke' 'dust' 'earthquake' 'east' 'elation'\n",
            " 'end' 'excursion' 'exempt' 'express' 'extraordinary' 'family' 'father'\n",
            " 'feel' 'feign' 'find' 'flabby' 'float' 'forever' 'forget' 'foul' 'found'\n",
            " 'frequently' 'fundamental' 'gatsby' 'generation' 'gesture' 'gift' 'give'\n",
            " 'glimpse' 'gorgeous' 'great' 'grief' 'habit' 'hard' 'heart' 'heighten'\n",
            " 'hope' 'horizon' 'hostile' 'housand' 'human' 'impressionability'\n",
            " 'inclined' 'infinite' 'interest' 'intimate' 'intricate' 'judgement'\n",
            " 'levity' 'life' 'like' 'likely' 'limit' 'little' 'machine' 'man' 'mar'\n",
            " 'marsh' 'matter' 'mean' 'middle' 'mile' 'mind' 'miss' 'moral' 'nature'\n",
            " 'normal' 'obvious' 'open' 'parcel' 'people' 'person' 'personality'\n",
            " 'plagiaristic' 'point' 'politician' 'preoccupation' 'prey' 'privileged'\n",
            " 'privy' 'prominent' 'promise' 'quality' 'quick' 'quiver' 'reaction'\n",
            " 'readiness' 'realize' 'register' 'relate' 'remember' 'repeat' 'represent'\n",
            " 'reserve' 'responsiveness' 'revelation' 'right' 'riotous' 'rock'\n",
            " 'romantic' 'scorn' 'secret' 'sense' 'sensitivity' 'series' 'shall'\n",
            " 'short' 'sign' 'sleep' 'snobbishly' 'sorrow' 'sort' 'successful'\n",
            " 'suggest' 'suppression' 'tell' 'temperament' 'temporarily' 'term'\n",
            " 'tolerance' 'tradition' 'turn' 'unaffected' 'unbroken' 'understand'\n",
            " 'unequally' 'uniform' 'unjustly' 'unknown' 'unmistakable' 'unsought'\n",
            " 'unusually' 'usually' 'veteran' 'victim' 'vulnerable' 'wake' 'want' 'way'\n",
            " 'western' 'wet' 'wild' 'wind' 'world' 'year' 'young']\n",
            "\n",
            "Great Gatsby TfidfVectorizer feature names:\n",
            " ['abnormal' 'abortive' 'accuse' 'actual' 'admission' 'advantage' 'advice'\n",
            " 'afraid' 'appear' 'attach' 'attention' 'autumn' 'away' 'birth' 'boast'\n",
            " 'book' 'bore' 'buccleuch' 'care' 'carraway' 'certain' 'city' 'clan'\n",
            " 'close' 'college' 'come' 'communicative' 'conduct' 'confidence'\n",
            " 'consequence' 'creative' 'criticize' 'curious' 'deal' 'decency' 'descend'\n",
            " 'detect' 'dignified' 'dream' 'duke' 'dust' 'earthquake' 'east' 'elation'\n",
            " 'end' 'excursion' 'exempt' 'express' 'extraordinary' 'family' 'father'\n",
            " 'feel' 'feign' 'find' 'flabby' 'float' 'forever' 'forget' 'foul' 'found'\n",
            " 'frequently' 'fundamental' 'gatsby' 'generation' 'gesture' 'gift' 'give'\n",
            " 'glimpse' 'gorgeous' 'great' 'grief' 'habit' 'hard' 'heart' 'heighten'\n",
            " 'hope' 'horizon' 'hostile' 'housand' 'human' 'impressionability'\n",
            " 'inclined' 'infinite' 'interest' 'intimate' 'intricate' 'judgement'\n",
            " 'levity' 'life' 'like' 'likely' 'limit' 'little' 'machine' 'man' 'mar'\n",
            " 'marsh' 'matter' 'mean' 'middle' 'mile' 'mind' 'miss' 'moral' 'nature'\n",
            " 'normal' 'obvious' 'open' 'parcel' 'people' 'person' 'personality'\n",
            " 'plagiaristic' 'point' 'politician' 'preoccupation' 'prey' 'privileged'\n",
            " 'privy' 'prominent' 'promise' 'quality' 'quick' 'quiver' 'reaction'\n",
            " 'readiness' 'realize' 'register' 'relate' 'remember' 'repeat' 'represent'\n",
            " 'reserve' 'responsiveness' 'revelation' 'right' 'riotous' 'rock'\n",
            " 'romantic' 'scorn' 'secret' 'sense' 'sensitivity' 'series' 'shall'\n",
            " 'short' 'sign' 'sleep' 'snobbishly' 'sorrow' 'sort' 'successful'\n",
            " 'suggest' 'suppression' 'tell' 'temperament' 'temporarily' 'term'\n",
            " 'tolerance' 'tradition' 'turn' 'unaffected' 'unbroken' 'understand'\n",
            " 'unequally' 'uniform' 'unjustly' 'unknown' 'unmistakable' 'unsought'\n",
            " 'unusually' 'usually' 'veteran' 'victim' 'vulnerable' 'wake' 'want' 'way'\n",
            " 'western' 'wet' 'wild' 'wind' 'world' 'year' 'young']\n"
          ]
        }
      ],
      "source": [
        "# Init count vectorizer\n",
        "count_vect = CountVectorizer()\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "\n",
        "# Fit transform on the corpus\n",
        "X_count_gatbsy = count_vect.fit_transform(corpus) # count vectorizer\n",
        "X_tfidf_gatsby = tfidf_vect.fit_transform(corpus) # Tfidfvectorizer\n",
        "\n",
        "# Get the feature names (tokens)\n",
        "gatsby_feat_names_count = count_vect.get_feature_names_out()\n",
        "gatsby_feat_names_tfidf = tfidf_vect.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(f\"Great Gatsby CountVectorizer feature names:\\n {gatsby_feat_names_count}\")\n",
        "print(f\"\\nGreat Gatsby TfidfVectorizer feature names:\\n {gatsby_feat_names_tfidf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWKY0ri9jHa4"
      },
      "outputs": [],
      "source": [
        "# Fit the vectorizers of your corpus obtain the feature names and the matrices\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5nCm9QL5qfj",
        "outputId": "ad1a3558-d943-4a65-f771-b6c8c6ac68ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CountVectorizer document-term matrix:\n",
            "[[0 0 0 ... 1 1 1]\n",
            " [1 0 1 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]]\n",
            "TfidfVectorizer document-term matrix:\n",
            "[[0.         0.         0.         ... 0.16615829 0.20017    0.16615829]\n",
            " [0.16299733 0.         0.16299733 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.13173276]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.19331996 0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Print the document-term matrices\n",
        "print(\"CountVectorizer document-term matrix:\")\n",
        "print(X_count_gatbsy.toarray())\n",
        "\n",
        "print(\"\\nTfidfVectorizer document-term matrix:\")\n",
        "print(X_tfidf_gatsby.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tza-pQqKeaJF"
      },
      "source": [
        "What is the noticeable difference between the two vectorization techniques?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHvHfI99BPaJ",
        "outputId": "5e3037ae-a04e-4e54-b99a-7c1dd6be47bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1: The term 'reserve' has the highest TF-IDF score of 0.3323165859445813\n",
            "Document 2: The term 'abnormal' has the highest TF-IDF score of 0.16299732751649293\n",
            "Document 3: The term 'revelation' has the highest TF-IDF score of 0.31739550733042693\n",
            "Document 4: The term 'want' has the highest TF-IDF score of 0.358395771054583\n",
            "Document 5: The term 'gatsby' has the highest TF-IDF score of 0.28916926679488314\n",
            "Document 6: The term 'find' has the highest TF-IDF score of 0.3843251299600989\n",
            "Document 7: The term 'abortive' has the highest TF-IDF score of 0.19331996379394867\n"
          ]
        }
      ],
      "source": [
        "# Get the TF-IDF scores for each document\n",
        "for i in range(len(corpus)):\n",
        "    tfidf_values = X_tfidf_gatsby[i].toarray().flatten()  # TF-IDF values for the i-th document\n",
        "    max_tfidf_index = tfidf_values.argmax()  # Index of the term with the highest TF-IDF score\n",
        "    max_tfidf_term = gatsby_feat_names_tfidf[max_tfidf_index]  # Term with the highest TF-IDF score\n",
        "    max_tfidf_score = tfidf_values[max_tfidf_index]  # Highest TF-IDF score\n",
        "    print(f\"Document {i+1}: The term '{max_tfidf_term}' has the highest TF-IDF score of {max_tfidf_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4rsMJqxeWVS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZPd4gujeT4Y"
      },
      "outputs": [],
      "source": [
        "# get the TF-IDF scores for your corpus of choice\n",
        "\n",
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}